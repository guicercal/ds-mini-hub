# üèõÔ∏è Arquitetura e Decis√µes T√©cnicas

*Leia a vers√£o em Ingl√™s: [ARCHITECTURE.md](ARCHITECTURE.md)*

Este documento explica as escolhas t√©cnicas do projeto DealSmart AI Communications Hub. O objetivo foi criar uma aplica√ß√£o robusta, de f√°cil manuten√ß√£o e pronta para o n√≠vel de produ√ß√£o.

---

## 1. Integra√ß√£o de IA: Padr√µes Strategy & Factory

Para evitar a codifica√ß√£o r√≠gida (hardcoding) de chamadas HTTP nas rotas do Next.js, o m√≥dulo de IA (`src/lib/ai/`) foi constru√≠do utilizando *Design Patterns* tradicionais.

- **Padr√£o Strategy:** Criamos a interface [AiProvider](src/lib/ai/types.ts#L14) que define o contrato: `generateSuggestion(messages)`. As classes `OpenAiProvider`, `ClaudeProvider`, e `GeminiProvider` implementam essa interface para lidar com as peculiaridades e integra√ß√µes de cada modelo (OpenAI, Anthropic, Google).
- **Padr√£o Factory:** As rotas n√£o inicializam provedores diretamente. Elas acionam a f√°brica [AiFactory](src/lib/ai/AiFactory.ts#L11). A f√°brica consulta o banco local (`AppSettings`) para checar qual o modelo ativo pelo usu√°rio, verifica as chaves configuradas no `.env` e retorna a inst√¢ncia adequada da IA.
- **Fail-Safe:** Caso o banco falhe, um `try...catch` garante que a Factory [retorne um modelo padr√£o (Gemini)](src/lib/ai/AiFactory.ts#L46), impedindo a aplica√ß√£o de travar.

## 2. Fluidez de Chat: Optimistic UI vs WebSockets

Em um ambiente serverless (como a Vercel), manter conex√µes WebSockets persistentes gera complexidade t√©cnica e requer servi√ßos adicionais dependentes. Por isso, optamos por **Short-Polling combinado a Optimistic UI**.

- **O Problema:** Um polling simples ([efetuar fetch a cada 3 segundos](src/components/ChatArea.tsx#L54)) causa falhas visuais. Ao enviar uma mensagem, ela aparece na tela, desaparece no pr√≥ximo poll caso n√£o tenha salvado a tempo no banco e, e reaparece em seguida (efeito "flicker").
- **A Solu√ß√£o:** Marcamos as mensagens locais rec√©m-enviadas com a flag [`isOptimistic = true` no ChatArea.tsx](src/components/ChatArea.tsx#L81). Ao receber uma nova lista do backend, o frontend inteligentemente [mescla o JSON real com essas mensagens tempor√°rias locais](src/components/ChatArea.tsx#L37).
- **O Resultado:** A interface exibe a mensagem de imediato simulando a velocidade de um WebSocket, enquanto a arquitetura permanece leve via chamadas HTTP REST pontuais.

## 3. Rate Limit: Cache em Mem√≥ria (TTL)

Acessar a API da HubSpot a cada requisi√ß√£o originada pelo polling exauriria o plano de acesso da conta em API rapidamente, resultando no erro estrutural "429 Too Many Requests".

Para resolver isso de forma simples, inserimos um `Cache em Mem√≥ria (LRU) com TTL` ([src/lib/hubspot-cache.ts](src/lib/hubspot-cache.ts#L32)).
- Quando os dados de Hubspot s√£o solicitados pelo frontend, o servidor intercepta a requisi√ß√£o.
- Tendo os dados do contato j√° cacheados no Node.js `Map` como [mais recentes que 60 segundos](src/lib/hubspot-cache.ts#L54), o servidor devolve a resposta em `~2ms` sem fazer nenhuma comunica√ß√£o √† API externa.
- Se o limite de tempo estourar (Time-To-Live), ele libera uma requisi√ß√£o fresca √† HubSpot e j√° [atualiza o objeto no cache](src/lib/hubspot-cache.ts#L69).
- **Resultado:** Mesmo se houverem v√°rias abas do navegador fazendo consultas simult√¢neas na rota [`/api/conversations/[id]`](src/app/api/conversations/[id]/route.ts#L39), a HubSpot √© consumida invariavelmente apenas uma vez por minuto para cada perfil de contato.

## 4. Seguran√ßa de Credenciais

- As chaves de API nunca s√£o expostas ao ambiente do navegador (client-side). Todo c√°lculo para prompts e consumos das APIs s√£o realizados estritamente nas rotas bloqueadas de Backend (`/api/`).
- O Frontend verifica quais Intelig√™ncias Artificiais est√£o dispon√≠veis globalmente via uma rota pretoriana independente ([`api/settings/ai/route.ts`](src/app/api/settings/ai/route.ts#L14)), e apenas carrega os seletores (dropdown) de nomes permitidos no c√≥digo fonte do Browser.

## 5. Testes Unit√°rios (Vitest)

Utilizamos o **Vitest** pela sua integra√ß√£o moderna, velocidade e suporte nativo pelo compilador Vite/ESM (evitando overhead de configura√ß√µes).
- Foco da su√≠te reside na l√≥gica de Core Business, e n√£o na inespecificidade visual de componentes CSS.
- Utilizamos `vi.mock` mitigando efeitos colaterais e blindando os Testes do Banco de Dados real (Prisma) e da Rede Externa.
- Para comprovar o bloqueio contra requisi√ß√µes abusivas da Rate Limit, a infra domina o tempo virtual invocando [vi.setSystemTime() para simular o avan√ßo de 61 segundos](src/lib/__tests__/hubspot-cache.test.ts#L52), atestando a caducidade do Cache Local sem recorrer a congelamentos for√ßados duradouros (como `sleep()` ou `setTimeout()`) nos processos ass√≠ncronos de pipeline de CI/CD.

## 6. Sugest√£o de Respostas com IA & Gerenciamento de Tokens

A funcionalidade de sugest√£o de respostas atua como o motor de intelig√™ncia do CRM, analisando o hist√≥rico da conversa para gerar respostas contextuais.

### Funcionamento e Contexto
Para garantir que a Intelig√™ncia Artificial compreenda a linha do tempo da negocia√ß√£o, as seguintes regras foram implementadas:
- **Consci√™ncia de Di√°logo (Context Awareness):** As mensagens s√£o mapeadas e convertidas para os pap√©is de `User` (Cliente) e `Assistant/Model` (Vendedor). Isso assegura que o modelo de linguagem n√£o perca o contexto de quem disse o qu√™.
- **System Prompts:** Antes de enviar a requisi√ß√£o ao LLM (OpenAI, Gemini ou Claude), a classe provedora [injeta uma instru√ß√£o de sistema (`System Prompt`)](src/lib/ai/OpenAiProvider.ts#L25). Esta instru√ß√£o define a "persona" da IA, garantindo que as respostas mantenham o tom de voz profissional e orientado a vendas da empresa.

### Otimiza√ß√£o e Controle de Tokens
APIs de LLM precificam requisi√ß√µes com base na contagem de Tokens (volume de texto trafegado). Para evitar custos operacionais fora de controle e otimizar a performance da rede, aplicamos dois limitadores:
1. **Fatiamento de Contexto (Window Slicing):** Enviar o hist√≥rico de conversas longas na √≠ntegra √© invi√°vel financeiramente e eleva a lat√™ncia. O componente frontend interv√©m e [aplica uma opera√ß√£o de `slice(-10)`](src/components/ChatArea.tsx#L124). Apenas as 10 intera√ß√µes mais recentes s√£o enviadas √† API, mantendo a "mem√≥ria de curto prazo" intacta ao mesmo tempo em que reduz massivamente o tamanho do payload.
2. **Limite Fixo de Resposta (`max_tokens`):** As requisi√ß√µes HTTP enviadas aos provedores estipulam explicitamente `maxOutputTokens: 1000`. Essa configura√ß√£o age como um limite definitivo no backend, garantindo respostas coesas para a interface de chat e prevenindo anomalias na gera√ß√£o excessiva de novos *tokens*.
